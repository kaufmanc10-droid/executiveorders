cleaned <- clean_eo_text(raw)
cleaned <- normalize_text_utf8(cleaned)
cleaned <- dehyphenate(cleaned)
out <- file.path("executive_orders_cleaned",
str_replace(basename(fp), "\\.txt$", "_cleaned.txt"))
write_file(cleaned, out)
}
cleaned_files <- list.files("executive_orders_cleaned", "_cleaned\\.txt$", full.names = TRUE)
cat(glue("Cleaned files: {length(cleaned_files)}\n"))
# ----------------------------------------------------------------
# 4) Chunking (word-level alignment; dual text)
# ----------------------------------------------------------------
count_tokens_est <- function(text) {
wc <- str_count(text, "\\S+")
as.integer(round(wc * 1.3))
}
chunk_document <- function(text_original, eo_number, max_words = 500) {
text_clean <- str_to_lower(text_original)
w_clean <- str_split(text_clean,  "\\s+")[[1]]
w_orig  <- str_split(text_original,"\\s+")[[1]]
if (length(w_clean) != length(w_orig)) {
n <- min(length(w_clean), length(w_orig))
w_clean <- w_clean[seq_len(n)]
w_orig  <- w_orig[seq_len(n)]
}
sents_clean <- tokenize_sentences(text_clean, strip_punct = FALSE, lowercase = FALSE)[[1]]
if (length(sents_clean) == 0) return(tibble())
chunks <- list(); cur <- NULL; pos <- 1; idx <- 0
for (s in sents_clean) {
sw <- str_count(s, "\\S+"); if (sw == 0) next
if (is.null(cur)) {
idx <- idx + 1; cur <- list(st = pos, en = pos + sw - 1, wc = sw)
} else if (cur$wc + sw > max_words) {
chunks[[length(chunks)+1]] <- cur
cur <- list(st = pos, en = pos + sw - 1, wc = sw)
} else {
cur$en <- pos + sw - 1; cur$wc <- cur$wc + sw
}
pos <- pos + sw
}
if (!is.null(cur)) chunks[[length(chunks)+1]] <- cur
purrr::map2_dfr(chunks, seq_along(chunks), function(ch, i) {
st <- ch$st; en <- min(ch$en, length(w_clean))
oc <- paste(w_orig[st:en],  collapse = " ")
lc <- paste(w_clean[st:en], collapse = " ")
tibble(
doc_id        = glue("EO_{eo_number}"),
eo_number     = as.numeric(eo_number),
chunk_id      = i,
title         = NA_character_,
signing_date  = NA,
text_original = oc,
text          = lc,
word_count    = en - st + 1,
token_count   = count_tokens_est(lc)
)
})
}
# Chunk all cleaned files
chunks_list <- list()
for (fp in cleaned_files) {
eo_num <- str_extract(basename(fp), "\\d+")
txt_o  <- read_file(fp)
ch <- chunk_document(txt_o, eo_num, max_words = 500)
if (nrow(ch)) chunks_list[[eo_num]] <- ch
}
chunks <- bind_rows(chunks_list)
# ----------------------------------------------------------------
# 5) Enrich with metadata + write CSV
# ----------------------------------------------------------------
eo_key <- meta %>% select(executive_order_number, title, signing_date)
chunks <- chunks %>%
left_join(eo_key, by = c("eo_number" = "executive_order_number")) %>%
mutate(
title        = coalesce(title.y, title.x),
signing_date = coalesce(signing_date.y, signing_date.x)
) %>%
select(doc_id, eo_number, chunk_id, title, signing_date,
text_original, text, word_count, token_count)
# Final normalization (vectorized safely)
normalize_text_utf8_vec <- function(x) vapply(x, normalize_text_utf8, character(1))
dehyphenate_vec <- function(x) vapply(x, dehyphenate, character(1))
for (col in c("title","text_original","text")) {
if (is.character(chunks[[col]])) {
chunks[[col]] <- normalize_text_utf8_vec(chunks[[col]])
chunks[[col]] <- dehyphenate_vec(chunks[[col]])
}
}
write_csv(chunks, "eo_chunks_final.csv")
cat(glue("‚úì Wrote eo_chunks_final.csv ({nrow(chunks)} chunks)\n"))
# ----------------------------------------------------------------
# 6) Build self-contained HTML UI (Base64 JSON embedded)
# ----------------------------------------------------------------
json_compact <- jsonlite::toJSON(chunks, dataframe = "rows",
auto_unbox = TRUE, pretty = FALSE, na = "null")
b64 <- base64enc::base64encode(charToRaw(json_compact))
write_ui_html <- function(b64, out = "EO_UI.html") {
con <- file(out, open = "wb")
emit <- function(...) writeLines(paste0(...), con = con, sep = "", useBytes = TRUE)
emit('<!DOCTYPE html>\n<html lang="en"><head>\n<meta charset="UTF-8">\n')
emit('<meta name="viewport" content="width=device-width, initial-scale=1.0">\n')
emit('<title>Executive Orders Retrieval System</title>\n<style>\n')
emit('*{margin:0;padding:0;box-sizing:border-box}\n')
emit('body{font-family:-apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,sans-serif;')
emit('background:linear-gradient(135deg,#667eea 0%,#764ba2 100%);min-height:100vh;padding:20px}\n')
emit('.container{max-width:1200px;margin:0 auto}\n')
emit('.header{background:#fff;padding:40px;border-radius:15px;box-shadow:0 10px 30px rgba(0,0,0,.2);margin-bottom:30px;text-align:center}\n')
emit('.header h1{color:#667eea;font-size:2.5em;margin-bottom:10px}.header p{color:#666;font-size:1.1em}\n')
emit('.search-box{background:#fff;padding:30px;border-radius:15px;box-shadow:0 10px 30px rgba(0,0,0,.2);margin-bottom:30px}\n')
emit('.search-input-group{display:flex;gap:10px}.search-input{flex:1;padding:15px;font-size:16px;border:2px solid #ddd;border-radius:8px}\n')
emit('.search-input:focus{outline:none;border-color:#667eea}.search-button{padding:15px 40px;background:#667eea;color:#fff;border:none;border-radius:8px;font-size:16px;font-weight:600;cursor:pointer}\n')
emit('.search-button:hover{background:#5568d3}.search-button:disabled{background:#ccc;cursor:not-allowed}\n')
emit('.loading{text-align:center;padding:40px;background:#fff;border-radius:15px;box-shadow:0 10px 30px rgba(0,0,0,.2)}\n')
emit('.results-container{margin-bottom:30px}.results-header{background:#fff;padding:20px 30px;border-radius:15px 15px 0 0;color:#333;font-size:1.2em;font-weight:600}\n')
emit('.result-card{background:#fff;padding:30px;margin-bottom:2px;box-shadow:0 2px 10px rgba(0,0,0,.1)}.result-card:last-child{border-radius:0 0 15px 15px;margin-bottom:0}\n')
emit('.result-header{display:flex;justify-content:space-between;align-items:flex-start;margin-bottom:15px;padding-bottom:15px;border-bottom:2px solid #f0f0f0}\n')
emit('.result-title{font-size:1.4em;color:#333;font-weight:600;flex:1}\n')
emit('.similarity-badge{background:linear-gradient(135deg,#667eea 0%,#764ba2 100%);color:#fff;padding:8px 16px;border-radius:20px;font-weight:600;font-size:.9em;white-space:nowrap;margin-left:20px}\n')
emit('.metadata{display:flex;gap:30px;margin-bottom:20px;flex-wrap:wrap}.metadata-item{color:#666;font-size:.95em}.metadata-item strong{color:#333}\n')
emit('.chunk-text{background:#f8f9fa;padding:20px;border-radius:8px;line-height:1.8;color:#444;border-left:4px solid #667eea;white-space:pre-wrap}\n')
emit('.stats-box{background:#fff;padding:30px;border-radius:15px;box-shadow:0 10px 30px rgba(0,0,0,.2)}\n')
emit('.stats-box h3{color:#667eea;margin-bottom:15px}.stats-box p{color:#666;margin-bottom:8px;font-size:.95em}\n')
emit('.example-queries{background:#f8f9fa;padding:15px;border-radius:8px;margin-top:15px}.example-query{display:inline-block;background:#fff;padding:5px 12px;border-radius:15px;margin:5px;cursor:pointer;border:1px solid #ddd}\n')
emit('.example-query:hover{background:#667eea;color:#fff;border-color:#667eea}.no-results{background:#fff;padding:40px;border-radius:15px;text-align:center;color:#666}\n')
emit('#debugPanel{margin-top:20px;background:#111;color:#eee;border-radius:10px;padding:15px;font-family:ui-monospace,SFMono-Regular,Menlo,Consolas,monospace;box-shadow:0 10px 30px rgba(0,0,0,.2)}\n')
emit('.hl{background:#fffb8f}\n')
emit('</style>\n</head><body><div class="container">\n')
emit('<div class="header"><h1>üîç Executive Orders Retrieval System</h1><p id="dataset-info">Loading dataset...</p></div>\n')
emit('<div class="search-box"><h2>Enter Your Search Query</h2>\n<div class="search-input-group">\n')
emit('<input type="text" id="searchInput" class="search-input" placeholder="e.g., immigration policy, energy subsidies, federal government..." onkeypress="if(event.key===\'Enter\') performSearch()">\n')
emit('<button class="search-button" onclick="performSearch()">Search</button>\n</div>\n')
emit('<div class="example-queries"><strong>Try these examples:</strong>\n')
emit('<span class="example-query" onclick="setQuery(\'immigration policy\')">immigration policy</span>\n')
emit('<span class="example-query" onclick="setQuery(\'energy subsidies\')">energy subsidies</span>\n')
emit('<span class="example-query" onclick="setQuery(\'federal government\')">federal government</span>\n')
emit('<span class="example-query" onclick="setQuery(\'tax credits\')">tax credits</span>\n')
emit('</div></div>\n')
emit('<div id="resultsContainer"></div>\n')
emit('<div class="stats-box" style="margin-top:30px;">\n<h3>About This System</h3>\n')
emit('<p><strong>Dataset:</strong> Trump Executive Orders (January‚ÄìJuly 2025)</p>\n')
emit('<p><strong>Total Chunks:</strong> <span id="totalChunks">-</span></p>\n')
emit('<p><strong>Executive Orders:</strong> <span id="totalEOs">-</span></p>\n')
emit('<p><strong>Retrieval Method:</strong> TF-IDF with Cosine Similarity</p>\n')
emit('<p><strong>Implementation:</strong> Pure JavaScript (no server required)</p>\n</div>\n')
emit('<div id="debugPanel"><div style="font-weight:700;color:#67e8f9">DEBUG</div><pre id="debugLog" style="white-space:pre-wrap;margin-top:8px;max-height:240px;overflow:auto;"></pre></div>\n')
emit('</div>\n<script>\n')
# Data loader with Base64
emit('function __LOAD_CHUNKS_DATA_FROM_BASE64__(){const el=document.getElementById("debugLog");const log=(...a)=>{if(el) el.textContent += a.map(x=>typeof x==="object"?JSON.stringify(x).slice(0,400):String(x)).join(" ")+"\\n";};')
emit('const b64="'); emit(b64); emit('";try{const jsonStr=atob(b64);log("Decoded JSON bytes:",jsonStr.length);const d=JSON.parse(jsonStr);log("Parsed length:",d.length);return Array.isArray(d)?d:[]}catch(e){log("Decode/parse error:",e&&e.message?e.message:e);return[]}}\n')
emit('const CHUNKS_DATA=__LOAD_CHUNKS_DATA_FROM_BASE64__();\n')
emit('const dbg=(...a)=>{const el=document.getElementById("debugLog");if(el) el.textContent += a.map(x=>typeof x==="object"?JSON.stringify(x).slice(0,400):String(x)).join(" ")+"\\n";};\n')
# TF-IDF retriever
emit('class TfIdfRetriever{constructor(d){this.documents=d;this.vocabulary=new Set();this.idf={};this.tfidfVectors=[];this.buildIndex()}')
emit('tokenize(t){if(!t)return[];return String(t).toLowerCase().replace(/[^\\w\\s]/g," ").split(/\\s+/).filter(x=>x.length>2)}')
emit('buildIndex(){const df={};this.documents.forEach(doc=>{const toks=this.tokenize(doc.text);const uniq=new Set(toks);uniq.forEach(term=>{this.vocabulary.add(term);df[term]=(df[term]||0)+1})});const N=this.documents.length;this.vocabulary.forEach(term=>{this.idf[term]=Math.log(N/(df[term]||1))});this.documents.forEach(doc=>{const toks=this.tokenize(doc.text);const tf={};toks.forEach(t=>{tf[t]=(tf[t]||0)+1});const vec={};for(const t in tf){vec[t]=tf[t]*(this.idf[t]||0)}this.tfidfVectors.push(vec)})}')
emit('cosine(a,b){let dot=0,m1=0,m2=0;const terms=new Set([...Object.keys(a),...Object.keys(b)]);terms.forEach(t=>{const x=a[t]||0,y=b[t]||0;dot+=x*y;m1+=x*x;m2+=y*y});if(!m1||!m2)return 0;return dot/(Math.sqrt(m1)*Math.sqrt(m2))}')
emit('search(q,topN=3){const toks=this.tokenize(q);const tf={};toks.forEach(t=>{tf[t]=(tf[t]||0)+1});const qv={};for(const t in tf){qv[t]=tf[t]*(this.idf[t]||0)}const sims=this.tfidfVectors.map((vec,i)=>({index:i,similarity:this.cosine(qv,vec),document:this.documents[i]}));sims.sort((a,b)=>b.similarity-a.similarity);return sims.slice(0,topN)}}\n')
# App logic
emit('let retriever=null;function initializeApp(){dbg("Boot:",{len:CHUNKS_DATA.length});if(!CHUNKS_DATA.length){document.getElementById("dataset-info").textContent="‚ö†Ô∏è No data embedded.";document.querySelector(".search-button").disabled=true;return}console.time("build-index");retriever=new TfIdfRetriever(CHUNKS_DATA);console.timeEnd("build-index");const eos=new Set(CHUNKS_DATA.map(d=>d.eo_number)).size;document.getElementById("dataset-info").textContent=`Search across ${CHUNKS_DATA.length} chunks from ${eos} Executive Orders`;document.getElementById("totalChunks").textContent=CHUNKS_DATA.length;document.getElementById("totalEOs").textContent=eos}')
emit('function setQuery(q){document.getElementById("searchInput").value=q;performSearch()}')
emit('function highlight(s,q){if(!q)return s;const words=String(q).toLowerCase().split(/\\s+/).filter(x=>x.length>2);let out=s||"";words.forEach(w=>{const re=new RegExp("("+w.replace(/[.*+?^${}()|[\\]\\\\]/g,"\\\\$&")+")","ig");out=out.replace(re,"<span class=\\"hl\\">$1</span>")});return out}')
emit('function performSearch(){const q=document.getElementById("searchInput").value.trim();if(!q){alert("Please enter a search query");return}if(!retriever){alert("System not initialized");return}const box=document.getElementById("resultsContainer");box.innerHTML=\'<div class="loading">üîç Searching...</div>\';setTimeout(()=>{let res=[];try{res=retriever.search(q,3)}catch(e){dbg("search ERROR:",e&&e.message?e.message:e)}displayResults(q,res)},40)}')
emit('function displayResults(q,res){const box=document.getElementById("resultsContainer");if(!res.length||res[0].similarity===0){box.innerHTML=`<div class="no-results"><h3>No results for "${q}"</h3><p>Try different terms.</p></div>`;return}let html=\'<div class="results-container">\';html+=`<div class="results-header">Top 3 Results for: "${q}"</div>`;res.forEach((r,i)=>{const d=r.document;const pct=(r.similarity*100).toFixed(1);const date=d.signing_date?new Date(d.signing_date):null;const dateStr=(date&&!isNaN(date))?date.toLocaleDateString("en-US",{year:"numeric",month:"long",day:"numeric"}):"‚Äî";let snip=d.text_original||d.text||"";if(snip.length>1200) snip=snip.slice(0,1200)+"...";snip=highlight(snip,q);html+=`<div class="result-card"><div class="result-header"><div class="result-title">#${i+1}: ${d.title||"(Untitled EO)"}</div><div class="similarity-badge">${pct}% match</div></div><div class="metadata"><div class="metadata-item"><strong>EO Number:</strong> ${d.eo_number??"‚Äî"}</div><div class="metadata-item"><strong>Date:</strong> ${dateStr}</div><div class="metadata-item"><strong>Chunk:</strong> ${d.chunk_id??"‚Äî"}</div><div class="metadata-item"><strong>Words:</strong> ${d.word_count??"‚Äî"}</div></div><div class="chunk-text">${snip}</div></div>`});html+=\'</div>\';box.innerHTML=html}')
emit('window.addEventListener("DOMContentLoaded", initializeApp);\n</script></body></html>\n')
close(con)
}
write_ui_html(b64)
cat("‚úì UI written to EO_UI.html\n")
# ----------------------------------------------------------------
# 7) Summary
# ----------------------------------------------------------------
rng <- range(meta$signing_date, na.rm = TRUE)
cat("\n=================================================\n")
cat("‚úì Pipeline complete\n")
cat("- CSV: eo_chunks_final.csv\n")
cat("- UI : EO_UI.html (self-contained; open in browser)\n")
cat(glue("- Corpus: {nrow(chunks)} chunks from {n_distinct(chunks$eo_number)} EOs ({rng[1]} to {rng[2]})\n"))
cat("=================================================\n")
library(dplyr)
library(stringr)
# Basic sanity checks
nrow(chunks)
names(chunks)
summary(chunks$text)
mean(str_count(chunks$text, "\\w+"), na.rm = TRUE)
sum(is.na(chunks$text))
cat(substr(chunks$text[1], 1, 400))
# ================================================================
# Executive Orders Retrieval System ‚Äî Full Pipeline + UI (Self-Contained)
# Output: eo_chunks_final.csv  and  EO_UI.html
# ================================================================
suppressPackageStartupMessages({
library(tidyverse)
library(pdftools)
library(tokenizers)
library(jsonlite)
library(glue)
library(base64enc)
library(readr)
library(stringr)
})
# ------------------------------- 0) Input checks
if (!file.exists("metadata.csv")) stop("metadata.csv not found in working directory.")
meta <- read_csv("metadata.csv", show_col_types = FALSE)
needed <- c("pdf_url","executive_order_number","title","signing_date")
miss <- setdiff(needed, names(meta))
if (length(miss)) stop(glue("metadata.csv missing columns: {paste(miss, collapse=', ')}"))
meta <- meta %>%
mutate(
executive_order_number = suppressWarnings(as.numeric(executive_order_number)),
signing_date = suppressWarnings(as.Date(signing_date))
)
dl_links <- meta %>%
filter(!is.na(pdf_url), !is.na(executive_order_number)) %>%
distinct(executive_order_number, .keep_all = TRUE)
cat(glue("Found {nrow(dl_links)} EOs with usable pdf_url\n"))
# ------------------------------- 1) Dirs
dir.create("executive_orders_pdf", showWarnings = FALSE)
dir.create("executive_orders_txt", showWarnings = FALSE)
dir.create("executive_orders_cleaned", showWarnings = FALSE)
# ------------------------------- 2) Download + extract
download_with_retries <- function(url, destfile, tries=3, sleep_sec=0.5){
for (i in seq_len(tries)) {
try(suppressWarnings(download.file(url, destfile, mode="wb", quiet=TRUE)), silent = TRUE)
if (file.exists(destfile) && file.size(destfile) > 0) return(TRUE)
Sys.sleep(sleep_sec)
}
FALSE
}
get_and_save_pdf_text <- function(eo_number, pdf_url, title){
pdf_path <- file.path("executive_orders_pdf", glue("EO_{eo_number}.pdf"))
txt_path <- file.path("executive_orders_txt", glue("EO_{eo_number}.txt"))
if (file.exists(txt_path) && file.size(txt_path) > 0) {
cat(glue("‚úì (cached) EO {eo_number}\n")); return(invisible(TRUE))
}
if (!file.exists(pdf_path) || file.size(pdf_path) == 0) {
cat(glue("‚Üì Downloading EO {eo_number}: {str_sub(title %||% '', 1, 60)}\n"))
if (!download_with_retries(pdf_url, pdf_path)) {
cat(glue("‚úó Download failed EO {eo_number}\n")); return(invisible(FALSE))
}
}
txt <- ""
ok <- FALSE
try({
p <- pdf_text(pdf_path)
if (length(p)) { txt <- paste(p, collapse = "\n\n"); ok <- TRUE }
}, silent = TRUE)
if (!ok || !nzchar(txt)) { cat(glue("‚úó Extract failed EO {eo_number}\n")); return(invisible(FALSE)) }
write_file(txt, txt_path)
cat(glue("‚úì Saved text EO {eo_number}\n"))
invisible(TRUE)
}
ok_ct <- 0
for (i in seq_len(nrow(dl_links))) {
r <- dl_links[i,]
if (isTRUE(get_and_save_pdf_text(r$executive_order_number, r$pdf_url, r$title))) ok_ct <- ok_ct + 1
Sys.sleep(0.15)
}
all_txt <- list.files("executive_orders_txt", "^EO_\\d+\\.txt$", full.names = TRUE)
cat(glue("Text files present: {length(all_txt)}\n"))
# ------------------------------- 3) Cleaning (preserve case)
clean_eo_text <- function(raw_text){
txt <- raw_text
# drop whole lines with these markers
kill_lines <- c("verdate","billing code","\\[fr doc","filed \\d","_prezdoc","presidential documents")
for (pat in kill_lines) {
txt <- str_remove_all(txt, regex(glue("^.*{pat}.*$"), multiline = TRUE, ignore_case = TRUE))
}
# headers/blocks
txt <- str_remove_all(txt, regex("federal register.*?presidential documents", ignore_case = TRUE, dotall = TRUE))
# doc codes / s codes
txt <- str_remove_all(txt, "\\b\\d{2}[a-z]{3}\\d{1,2}\\b")
txt <- str_remove_all(txt, "\\bs \\d{2}[a-z]{3}\\d{1,2}\\b")
txt <- str_replace_all(txt, "\\bs\\s+(?=\\d)", "")
# page numbers
txt <- str_remove_all(txt, regex("^\\s*\\d{4,6}\\s*$", multiline = TRUE))
# volume/date headers
txt <- str_remove_all(txt, regex("vol\\.\\s*\\d+,\\s*no\\.\\s*\\d+", ignore_case = TRUE))
txt <- str_remove_all(txt, regex("[a-z]+day,\\s*[a-z]+\\s*\\d+,\\s*\\d{4}", ignore_case = TRUE))
# metadata keywords
txt <- str_remove_all(txt, regex("jkt\\s+\\d+|po\\s+\\d+|frm\\s+\\d+|fmt\\s+\\d+|sfmt\\s+\\d+", ignore_case = TRUE))
# file paths/images
txt <- str_remove_all(txt, regex("e:\\\\fr\\\\fm\\\\.*?\\.sgm", ignore_case = TRUE))
txt <- str_remove_all(txt, regex("trump\\.eps</gph>|</?gph>", ignore_case = TRUE))
# White House closings
txt <- str_remove_all(txt, regex("the white house,?\\s*[a-z]*,?\\s*[a-z]+\\s*\\d+,\\s*\\d{4}\\.?", ignore_case = TRUE))
# prod lines
txt <- str_remove_all(txt, regex("[a-z]+\\s+on\\s+[a-z0-9]+prod\\s+with\\s+[a-z0-9-]+", ignore_case = TRUE))
# whitespace tidy
txt <- str_replace_all(txt, "\\n{3,}", "\n\n")
txt <- str_replace_all(txt, "[ \\t]+", " ")
txt <- str_replace_all(txt, regex("^\\s+$", multiline = TRUE), "")
txt <- str_trim(txt)
# final small sweep
txt <- str_remove_all(txt, "\\bs\\s+\\b|_prezdoc\\d+|\\b\\d{2}[a-z]{3}\\d{1,2}\\b")
str_trim(txt)
}
for (fp in all_txt) {
raw <- read_file(fp)
cleaned <- clean_eo_text(raw)
out <- file.path("executive_orders_cleaned", str_replace(basename(fp), "\\.txt$", "_cleaned.txt"))
write_file(cleaned, out)
}
cleaned_files <- list.files("executive_orders_cleaned", "_cleaned\\.txt$", full.names = TRUE)
cat(glue("Cleaned files: {length(cleaned_files)}\n"))
# ------------------------------- 4) Chunking (word-level alignment; dual text)
count_tokens_est <- function(text){
wc <- str_count(text, "\\S+"); as.integer(round(wc * 1.3))
}
chunk_document <- function(text_original, eo_number, target_words=400, max_words=500){
text_clean <- str_to_lower(text_original)
w_clean <- str_split(text_clean,  "\\s+")[[1]]
w_orig  <- str_split(text_original,"\\s+")[[1]]
if (length(w_clean) != length(w_orig)) {
n <- min(length(w_clean), length(w_orig))
w_clean <- w_clean[seq_len(n)]; w_orig <- w_orig[seq_len(n)]
}
sents_clean <- tokenize_sentences(text_clean, strip_punct = FALSE, lowercase = FALSE)[[1]]
if (length(sents_clean) == 0) return(tibble())
chunks <- list(); cur <- NULL; pos <- 1; idx <- 0
for (s in sents_clean) {
sw <- str_count(s, "\\S+"); if (sw==0) next
if (is.null(cur)) {
idx <- idx + 1; cur <- list(st=pos, en=pos+sw-1, wc=sw)
} else if (cur$wc + sw > max_words) {
chunks[[length(chunks)+1]] <- cur
cur <- list(st=pos, en=pos+sw-1, wc=sw)
} else {
cur$en <- pos+sw-1; cur$wc <- cur$wc + sw
}
pos <- pos + sw
}
if (!is.null(cur)) chunks[[length(chunks)+1]] <- cur
purrr::map2_dfr(chunks, seq_along(chunks), function(ch, i){
st <- ch$st; en <- min(ch$en, length(w_clean))
oc <- paste(w_orig[st:en], collapse=" ")
lc <- paste(w_clean[st:en], collapse=" ")
tibble(
doc_id        = glue("EO_{eo_number}"),
eo_number     = as.numeric(eo_number),
chunk_id      = i,
title         = NA_character_,
signing_date  = NA,
text_original = oc,
text          = lc,
word_count    = en - st + 1,
token_count   = count_tokens_est(lc)
)
})
}
chunks_list <- list()
for (fp in cleaned_files) {
eo_num <- str_extract(basename(fp), "\\d+")
txt_o  <- read_file(fp)
ch <- chunk_document(txt_o, eo_num, target_words=400, max_words=500)
if (nrow(ch)) chunks_list[[eo_num]] <- ch
}
chunks <- bind_rows(chunks_list)
# ------------------------------- 5) Enrich with metadata + CSV (UTF-8)
eo_key <- meta %>%
transmute(
executive_order_number = as.numeric(executive_order_number),
title,
signing_date
)
chunks <- chunks %>%
left_join(eo_key, by = c("eo_number" = "executive_order_number")) %>%
mutate(
title = coalesce(title.y, title.x),
signing_date = coalesce(signing_date.y, signing_date.x)
) %>%
select(doc_id, eo_number, chunk_id, title, signing_date, text_original, text, word_count, token_count)
# Normalize to UTF-8
utf8_cols <- function(df) {
df[] <- lapply(df, function(col) if (is.character(col)) enc2utf8(col) else col)
df
}
chunks <- utf8_cols(chunks)
# Optional: scrub common mojibake if any slipped in
fix_mojibake <- function(x) {
if (!is.character(x)) return(x)
x <- enc2utf8(x)
x <- gsub("√¢‚Ç¨‚Ñ¢", "‚Äô", x, fixed = TRUE)
x <- gsub("√¢‚Ç¨≈ì", "‚Äú", x, fixed = TRUE)
x <- gsub("√¢‚Ç¨ÔøΩ", "‚Äù", x, fixed = TRUE)
x <- gsub("√¢‚Ç¨Àú", "‚Äò", x, fixed = TRUE)
x <- gsub("√¢‚Ç¨‚Äù", "‚Äî", x, fixed = TRUE)
x <- gsub("√¢‚Ç¨‚Äú", "‚Äì", x, fixed = TRUE)
x <- gsub("√¢‚Ç¨¬¶", "‚Ä¶", x, fixed = TRUE)
x
}
for (c in c("title","text_original","text")) chunks[[c]] <- fix_mojibake(chunks[[c]])
write_csv(chunks, "eo_chunks_final.csv")
cat(glue("‚úì Wrote eo_chunks_final.csv ({nrow(chunks)} chunks)\n"))
# ------------------------------- 6) Build self-contained HTML (one method only)
write_ui_html <- function(b64, out = "EO_UI.html") {
con <- file(out, open = "wb")  # binary mode preserves UTF-8
emit <- function(...) writeLines(paste0(...), con = con, sep = "", useBytes = TRUE)
emit('<!DOCTYPE html>\n<html lang="en">\n<head>\n<meta charset="UTF-8">\n')
emit('<meta name="viewport" content="width=device-width, initial-scale=1.0">\n')
emit('<title>Executive Orders Retrieval System</title>\n<style>\n')
emit('*{margin:0;padding:0;box-sizing:border-box}\n')
emit('body{font-family:-apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,sans-serif;')
emit('background:linear-gradient(135deg,#667eea 0%,#764ba2 100%);min-height:100vh;padding:20px}\n')
emit('.container{max-width:1200px;margin:0 auto}\n')
emit('.header{background:#fff;padding:40px;border-radius:15px;box-shadow:0 10px 30px rgba(0,0,0,.2);margin-bottom:30px;text-align:center}\n')
emit('.header h1{color:#667eea;font-size:2.5em;margin-bottom:10px}\n')
emit('.header p{color:#666;font-size:1.1em}\n')
emit('.search-box{background:#fff;padding:30px;border-radius:15px;box-shadow:0 10px 30px rgba(0,0,0,.2);margin-bottom:30px}\n')
emit('.search-input-group{display:flex;gap:10px}\n')
emit('.search-input{flex:1;padding:15px;font-size:16px;border:2px solid #ddd;border-radius:8px}\n')
emit('.search-input:focus{outline:none;border-color:#667eea}\n')
emit('.search-button{padding:15px 40px;background:#667eea;color:#fff;border:none;border-radius:8px;font-size:16px;font-weight:600;cursor:pointer}\n')
emit('.search-button:hover{background:#5568d3}\n')
emit('.search-button:disabled{background:#ccc;cursor:not-allowed}\n')
emit('.loading{text-align:center;padding:40px;background:#fff;border-radius:15px;box-shadow:0 10px 30px rgba(0,0,0,.2)}\n')
emit('.results-container{margin-bottom:30px}\n')
emit('.results-header{background:#fff;padding:20px 30px;border-radius:15px 15px 0 0;color:#333;font-size:1.2em;font-weight:600}\n')
emit('.result-card{background:#fff;padding:30px;margin-bottom:2px;box-shadow:0 2px 10px rgba(0,0,0,.1)}\n')
emit('.result-card:last-child{border-radius:0 0 15px 15px;margin-bottom:0}\n')
emit('.result-header{display:flex;justify-content:space-between;align-items:flex-start;margin-bottom:15px;padding-bottom:15px;border-bottom:2px solid #f0f0f0}\n')
emit('.result-title{font-size:1.4em;color:#333;font-weight:600;flex:1}\n')
emit('.similarity-badge{background:linear-gradient(135deg,#667eea 0%,#764ba2 100%);color:#fff;padding:8px 16px;border-radius:20px;font-weight:600;font-size:.9em;white-space:nowrap;margin-left:20px}\n')
emit('.metadata{display:flex;gap:30px;margin-bottom:20px;flex-wrap:wrap}\n')
emit('.metadata-item{color:#666;font-size:.95em}\n')
emit('.metadata-item strong{color:#333}\n')
emit('.chunk-text{background:#f8f9fa;padding:20px;border-radius:8px;line-height:1.8;color:#444;border-left:4px solid #667eea;white-space:pre-wrap}\n')
emit('.stats-box{background:#fff;padding:30px;border-radius:15px;box-shadow:0 10px 30px rgba(0,0,0,.2)}\n')
emit('.stats-box h3{color:#667eea;margin-bottom:15px}\n')
emit('.stats-box p{color:#666;margin-bottom:8px;font-size:.95em}\n')
emit('.example-queries{background:#f8f9fa;padding:15px;border-radius:8px;margin-top:15px}\n')
emit('.example-query{display:inline-block;background:#fff;padding:5px 12px;border-radius:15px;margin:5px;cursor:pointer;border:1px solid #ddd}\n')
emit('.example-query:hover{background:#667eea;color:#fff;border-color:#667eea}\n')
emit('.no-results{background:#fff;padding:40px;border-radius:15px;text-align:center;color:#666}\n')
emit('#debugPanel{margin-top:20px;background:#111;color:#eee;border-radius:10px;padding:15px;font-family:ui-monospace,SFMono-Regular,Menlo,Consolas,monospace;box-shadow:0 10px 30px rgba(0,0,0,.2)}\n')
emit('.hl{background:#fffb8f}\n')
emit('</style>\n</head>\n<body>\n<div class="container">\n')
emit('<div class="header"><h1>üîç Executive Orders Retrieval System</h1><p id="dataset-info">Loading dataset...</p></div>\n')
emit('<div class="search-box"><h2>Enter Your Search Query</h2>\n')
emit('<div class="search-input-group">\n')
emit('<input type="text" id="searchInput" class="search-input" placeholder="e.g., immigration policy, energy subsidies, federal government..." onkeypress="if(event.key===\'Enter\') performSearch()">\n')
emit('<button class="search-button" onclick="performSearch()">Search</button>\n')
emit('</div>\n<div class="example-queries"><strong>Try these examples:</strong>\n')
emit('<span class="example-query" onclick="setQuery(\'immigration policy\')">immigration policy</span>\n')
emit('<span class="example-query" onclick="setQuery(\'energy subsidies\')">energy subsidies</span>\n')
emit('<span class="example-query" onclick="setQuery(\'federal government\')">federal government</span>\n')
emit('<span class="example-query" onclick="setQuery(\'tax credits\')">tax credits</span>\n')
emit('</div></div>\n')
emit('<div id="resultsContainer"></div>\n')
emit('<div class="stats-box" style="margin-top:30px;">\n')
emit('<h3>About This System</h3>\n')
emit('<p><strong>Dataset:</strong> Trump Executive Orders (January‚ÄìJuly 2025)</p>\n')
emit('<p><strong>Total Chunks:</strong> <span id="totalChunks">-</span></p>\n')
emit('<p><strong>Executive Orders:</strong> <span id="totalEOs">-</span></p>\n')
emit('<p><strong>Retrieval Method:</strong> TF-IDF with Cosine Similarity</p>\n')
emit('<p><strong>Implementation:</strong> Pure JavaScript (no server required)</p>\n')
emit('</div>\n')
emit('<div id="debugPanel"><div style="font-weight:700;color:#67e8f9">DEBUG</div><pre id="debugLog" style="white-space:pre-wrap;margin-top:8px;max-height:240px;overflow:auto;"></pre></div>\n')
emit('</div>\n')
# Script
emit('<script>\n')
emit('function __LOAD_CHUNKS_DATA_FROM_BASE64__(){\n')
emit('  const el=document.getElementById("debugLog");\n')
emit('  const log=(...a)=>{if(el) el.textContent += a.map(x=>typeof x==="object"?JSON.stringify(x).slice(0,400):String(x)).join(" ")+"\\n";};\n')
emit('  const b64="'); emit(b64); emit('";\n')
emit('  try{const jsonStr=atob(b64); log("Decoded JSON bytes:", jsonStr.length); const data=JSON.parse(jsonStr); log("Parsed length:", data.length); return Array.isArray(data)?data:[];}catch(e){log("Decode/parse error:", e&&e.message?e.message:e);return[]}\n')
emit('}\n')
emit('const CHUNKS_DATA = __LOAD_CHUNKS_DATA_FROM_BASE64__();\n')
emit('const dbg=(...a)=>{const el=document.getElementById("debugLog"); if(el) el.textContent += a.map(x=>typeof x==="object"?JSON.stringify(x).slice(0,400):String(x)).join(" ")+"\\n";};\n')
emit('class TfIdfRetriever{\n')
emit('  constructor(d){this.documents=d;this.vocabulary=new Set();this.idf={};this.tfidfVectors=[];this.buildIndex();}\n')
emit('  tokenize(t){if(!t)return[];return String(t).toLowerCase().replace(/[^\\w\\s]/g," ").split(/\\s+/).filter(x=>x.length>2)}\n')
emit('  buildIndex(){const df={};this.documents.forEach(doc=>{const toks=this.tokenize(doc.text);const uniq=new Set(toks);uniq.forEach(term=>{this.vocabulary.add(term);df[term]=(df[term]||0)+1;});});\n')
emit('    const N=this.documents.length;this.vocabulary.forEach(term=>{this.idf[term]=Math.log(N/(df[term]||1));});\n')
emit('    this.documents.forEach(doc=>{const toks=this.tokenize(doc.text);const tf={};toks.forEach(t=>{tf[t]=(tf[t]||0)+1});const vec={};for(const t in tf){vec[t]=tf[t]*(this.idf[t]||0)}this.tfidfVectors.push(vec);});}\n')
emit('  cosine(a,b){let dot=0,m1=0,m2=0;const terms=new Set([...Object.keys(a),...Object.keys(b)]);terms.forEach(t=>{const x=a[t]||0,y=b[t]||0;dot+=x*y;m1+=x*x;m2+=y*y;});if(!m1||!m2)return 0;return dot/(Math.sqrt(m1)*Math.sqrt(m2))}\n')
emit('  search(q,topN=3){const toks=this.tokenize(q);const tf={};toks.forEach(t=>{tf[t]=(tf[t]||0)+1});const qv={};for(const t in tf){qv[t]=tf[t]*(this.idf[t]||0)}\n')
emit('    const sims=this.tfidfVectors.map((vec,i)=>({index:i,similarity:this.cosine(qv,vec),document:this.documents[i]}));sims.sort((a,b)=>b.similarity-a.similarity);return sims.slice(0,topN)}\n')
emit('}\n')
emit('let retriever=null;\n')
emit('function initializeApp(){dbg("Boot:",{len:CHUNKS_DATA.length});if(!CHUNKS_DATA.length){document.getElementById("dataset-info").textContent="‚ö†Ô∏è No data embedded.";document.querySelector(".search-button").disabled=true;return;}\n')
emit('  console.time("build-index");retriever=new TfIdfRetriever(CHUNKS_DATA);console.timeEnd("build-index");\n')
emit('  const eos=new Set(CHUNKS_DATA.map(d=>d.eo_number)).size;document.getElementById("dataset-info").textContent=`Search across ${CHUNKS_DATA.length} chunks from ${eos} Executive Orders`;\n')
emit('  document.getElementById("totalChunks").textContent=CHUNKS_DATA.length;document.getElementById("totalEOs").textContent=eos;}\n')
emit('function setQuery(q){document.getElementById("searchInput").value=q;performSearch()}\n')
emit('function highlight(s,q){if(!q)return s;const words=String(q).toLowerCase().split(/\\s+/).filter(x=>x.length>2);let out=s||"";words.forEach(w=>{const re=new RegExp("("+w.replace(/[.*+?^${}()|[\\]\\\\]/g,"\\\\$&")+")","ig");out=out.replace(re,"<span class=\\"hl\\">$1</span>")});return out}\n')
emit('function performSearch(){const q=document.getElementById("searchInput").value.trim();if(!q){alert("Please enter a search query");return}if(!retriever){alert("System not initialized");return}\n')
emit('  const box=document.getElementById("resultsContainer");box.innerHTML=\'<div class="loading">üîç Searching...</div>\';setTimeout(()=>{let res=[];try{res=retriever.search(q,3)}catch(e){dbg("search ERROR:",e&&e.message?e.message:e)}displayResults(q,res)},50)}\n')
emit('function displayResults(q,res){const box=document.getElementById("resultsContainer");if(!res.length||res[0].similarity===0){box.innerHTML=`<div class="no-results"><h3>No results for "${q}"</h3><p>Try different terms.</p></div>`;return}\n')
emit('  let html=\'<div class="results-container">\';html+=`<div class="results-header">Top 3 Results for: "${q}"</div>`;res.forEach((r,i)=>{const d=r.document;const pct=(r.similarity*100).toFixed(1);const date=d.signing_date?new Date(d.signing_date):null;const dateStr=(date&&!isNaN(date))?date.toLocaleDateString("en-US",{year:"numeric",month:"long",day:"numeric"}):"‚Äî";\n')
emit('    let snip=d.text_original||d.text||"";if(snip.length>1200) snip=snip.slice(0,1200)+"...";snip=highlight(snip,q);\n')
emit('    html+=`<div class="result-card"><div class="result-header"><div class="result-title">#${i+1}: ${d.title||"(Untitled EO)"}</div><div class="similarity-badge">${pct}% match</div></div><div class="metadata"><div class="metadata-item"><strong>EO Number:</strong> ${d.eo_number??"‚Äî"}</div><div class="metadata-item"><strong>Date:</strong> ${dateStr}</div><div class="metadata-item"><strong>Chunk:</strong> ${d.chunk_id??"‚Äî"}</div><div class="metadata-item"><strong>Words:</strong> ${d.word_count??"‚Äî"}</div></div><div class="chunk-text">${snip}</div></div>`;\n')
emit('  }); html+=\'</div>\'; box.innerHTML=html }\n')
emit('window.addEventListener("DOMContentLoaded", initializeApp);\n')
emit('</script>\n</body>\n</html>\n')
close(con)
}
# Prepare Base64 JSON (force UTF-8 bytes)
json_compact <- toJSON(utf8_cols(chunks), dataframe = "rows", auto_unbox = TRUE,
pretty = FALSE, na = "null")
json_compact <- enc2utf8(json_compact)           # ensure UTF-8
b64 <- base64encode(charToRaw(json_compact))     # Base64 of UTF-8 bytes
write_ui_html(b64)
cat("‚úì UI written to EO_UI.html\n")
# ------------------------------- Summary
rng <- range(meta$signing_date, na.rm = TRUE)
cat("\n=================================================\n")
cat("‚úì Pipeline complete\n")
cat("- CSV: eo_chunks_final.csv\n")
cat("- UI : EO_UI.html (self-contained; open in browser)\n")
cat(glue("- Corpus: {nrow(chunks)} chunks from {n_distinct(chunks$eo_number)} EOs ({rng[1]} to {rng[2]})\n"))
cat("=================================================\n")
